{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManuSinghYadav/Andrej_Karpathy_Zero_to_Hero/blob/main/AK_Lecture_Series_Nano_GPT_(Full).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Andrej's Code](https://colab.sandbox.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=hoelkOrFY8bN)"
      ],
      "metadata": {
        "id": "gBpzhuCfuYfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrqB4Z-5Riut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f5914a-95a0-4f81-ef80-07c157e6de8a",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-10 07:14:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.6’\n",
            "\n",
            "\rinput.txt.6           0%[                    ]       0  --.-KB/s               \rinput.txt.6         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-07-10 07:14:25 (16.6 MB/s) - ‘input.txt.6’ saved [1115394/1115394]\n",
            "\n",
            "    0 / 10000 : train loss 4.1902, val loss 4.1902\n",
            "  500 / 10000 : train loss 2.5846, val loss 2.5954\n",
            " 1000 / 10000 : train loss 2.4651, val loss 2.4678\n",
            " 1500 / 10000 : train loss 2.4005, val loss 2.4000\n",
            " 2000 / 10000 : train loss 2.3406, val loss 2.3531\n",
            " 2500 / 10000 : train loss 2.3139, val loss 2.3329\n",
            " 3000 / 10000 : train loss 2.2982, val loss 2.3089\n",
            " 3500 / 10000 : train loss 2.2641, val loss 2.2855\n",
            " 4000 / 10000 : train loss 2.2536, val loss 2.2546\n",
            " 4500 / 10000 : train loss 2.2354, val loss 2.2460\n",
            " 5000 / 10000 : train loss 2.2250, val loss 2.2503\n",
            " 5500 / 10000 : train loss 2.1952, val loss 2.2350\n",
            " 6000 / 10000 : train loss 2.2113, val loss 2.2166\n",
            " 6500 / 10000 : train loss 2.1906, val loss 2.2066\n",
            " 7000 / 10000 : train loss 2.1868, val loss 2.2053\n",
            " 7500 / 10000 : train loss 2.1629, val loss 2.2035\n",
            " 8000 / 10000 : train loss 2.1624, val loss 2.2064\n",
            " 8500 / 10000 : train loss 2.1792, val loss 2.2054\n",
            " 9000 / 10000 : train loss 2.1653, val loss 2.1980\n",
            " 9500 / 10000 : train loss 2.1696, val loss 2.1997\n",
            "And ireivelf did; is worgrsed?\n",
            "\n",
            "ATLOUS:\n",
            "DUCHARD:\n",
            "Ey your and in alonghs kent Eithe whath malds\n",
            "Deearve der lom\n",
            "And krin\n",
            "Whist ringt in hou thouninod sie ackn of now into hereagr dows, yefreacus.\n",
            "Worpey,\n",
            "Aner thollave veadst as nath; of show.\n",
            "\n",
            "Ee; anls, lod'd wierrve\n",
            "Lord,\n",
            "Is for pon to is wart.\n",
            "When I lish.\n",
            "\n",
            "HE\n",
            "'Thes rand with thus, froer they cut menly me a kher. Witseost aweince, opfr,\n",
            "Do andsist he lows; not balsiarkn, whosore SE VIIZIO:\n",
            "Ser,\n",
            "In histy thit lof. Whan heartivestch thers for sty \n"
          ]
        }
      ],
      "source": [
        "# @title Version 1 (Without Blocks, Residual Layers and LayerNorm)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "learning_rate = 1e-3\n",
        "eval_interval = 500\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Getting batches\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    # Fix: Slice tril to match the current sequence length T\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads =  nn.ModuleList([Head(head_size) for h in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h (x) for h in self.heads], dim=2)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.fwd = nn.Sequential(\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fwd(x)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_head = MultiHeadAttention(4, n_embd//4)\n",
        "    self.fwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"idx (B, T), targets (B, T).\"\"\"\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.positional_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_head(x)\n",
        "    fwd = self.fwd(x)\n",
        "    logits = self.lm_head(fwd)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for i in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self.forward(idx_cond)\n",
        "      logits = logits[:,-1,:] # Picking the last row of each batch.\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "# Defining our model\n",
        "model = BigramLanguageModel().to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"{iter: 5d} / {max_iters} : train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  if iter == 8000:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate*0.00001)  # Learning rate decay\n",
        "\n",
        "# Printing output\n",
        "output = model.generate(torch.tensor([[13]], device=device), 500)\n",
        "print(decode(output[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Version 2 (With Blocks, Residual Layers and LayerNorm)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "learning_rate = 1e-3\n",
        "eval_interval = 500\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Getting batches\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    # Fix: Slice tril to match the current sequence length T\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads =  nn.ModuleList([Head(head_size) for h in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h (x) for h in self.heads], dim=2)\n",
        "    out = self.proj(out)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.fwd = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fwd(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sa(self.ln1(x))  # Residual Add\n",
        "    x = self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "        Block(n_embd, n_head=4),\n",
        "    )\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"idx (B, T), targets (B, T).\"\"\"\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.positional_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for i in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self.forward(idx_cond)\n",
        "      logits = logits[:,-1,:] # Picking the last row of each batch.\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "# Defining our model\n",
        "model = BigramLanguageModel().to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"{iter: 5d} / {max_iters} : train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  if iter == 8000:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate*0.00001)  # Learning rate decay\n",
        "\n",
        "# Printing output\n",
        "output = model.generate(torch.tensor([[13]], device=device), 500)\n",
        "print(decode(output[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "8ScqX41hKI2E",
        "outputId": "1e5e6b36-3319-41f1-a3a1-91b195b1d029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-11 07:22:44--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-07-11 07:22:45 (30.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "    0 / 10000 : train loss 3.9622, val loss 3.9798\n",
            "  500 / 10000 : train loss 2.7101, val loss 2.7011\n",
            " 1000 / 10000 : train loss 2.5202, val loss 2.4925\n",
            " 1500 / 10000 : train loss 2.4079, val loss 2.4091\n",
            " 2000 / 10000 : train loss 2.3565, val loss 2.3719\n",
            " 2500 / 10000 : train loss 2.2864, val loss 2.3039\n",
            " 3000 / 10000 : train loss 2.2626, val loss 2.2912\n",
            " 3500 / 10000 : train loss 2.2217, val loss 2.2403\n",
            " 4000 / 10000 : train loss 2.1802, val loss 2.2264\n",
            " 4500 / 10000 : train loss 2.1737, val loss 2.2275\n",
            " 5000 / 10000 : train loss 2.1387, val loss 2.1829\n",
            " 5500 / 10000 : train loss 2.1254, val loss 2.1619\n",
            " 6000 / 10000 : train loss 2.1035, val loss 2.1628\n",
            " 6500 / 10000 : train loss 2.0798, val loss 2.1638\n",
            " 7000 / 10000 : train loss 2.0758, val loss 2.1388\n",
            " 7500 / 10000 : train loss 2.0799, val loss 2.1506\n",
            " 8000 / 10000 : train loss 2.0663, val loss 2.1339\n",
            " 8500 / 10000 : train loss 2.0554, val loss 2.1317\n",
            " 9000 / 10000 : train loss 2.0690, val loss 2.1396\n",
            " 9500 / 10000 : train loss 2.0527, val loss 2.1304\n",
            "A hartaight right'd a stanttled then beglow.\n",
            "\n",
            "DUS:\n",
            "Yrince wittod!\n",
            "\n",
            "DUBK:\n",
            "Heal would it Ruack our fondolditble,\n",
            "We reevist, silt to handing\n",
            "The grioaghis, trow will be and ofethed he the wall wich fiel's withs fenb, fraint the my of usiottlunfer, work.\n",
            "\n",
            "ABARARBTA:\n",
            "You deel, is sarm.\n",
            "\n",
            "BNUS:\n",
            "Te sting my feds dieve dosive what sweal an's lontb,\n",
            "I'fer ray, from thou voper; wite-QY RAURTA:\n",
            "Of meot battinow mille-air vloty thou neatt her's so father shall cher, all you che the thy comple, I woopmen, of \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Version 3 (Scaling it up)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 10000\n",
        "learning_rate = 3e-3\n",
        "eval_interval = 500\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Getting batches\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    # Fix: Slice tril to match the current sequence length T\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads =  nn.ModuleList([Head(head_size) for h in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h (x) for h in self.heads], dim=2)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.fwd = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fwd(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sa(self.ln1(x))  # Residual Add\n",
        "    x = self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head) for i in range(3)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"idx (B, T), targets (B, T).\"\"\"\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
        "    pos_emb = self.positional_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for i in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self.forward(idx_cond)\n",
        "      logits = logits[:,-1,:] # Picking the last row of each batch.\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "# Defining our model\n",
        "model = BigramLanguageModel().to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"{iter: 5d} / {max_iters} : train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  if iter == 8000:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate*0.00001)  # Learning rate decay\n",
        "\n",
        "# Printing output\n",
        "output = model.generate(torch.tensor([[13]], device=device), 500)\n",
        "print(decode(output[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "ttsHBBhB7DsS",
        "outputId": "f039fd19-02b7-487a-9429-7e02982a6fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-11 09:43:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-07-11 09:43:51 (25.9 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "    0 / 10000 : train loss 4.4665, val loss 4.4996\n",
            "  500 / 10000 : train loss 3.3165, val loss 3.3520\n",
            " 1000 / 10000 : train loss 3.3105, val loss 3.3438\n",
            " 1500 / 10000 : train loss 3.3094, val loss 3.3464\n",
            " 2000 / 10000 : train loss 3.3104, val loss 3.3499\n",
            " 2500 / 10000 : train loss 3.3117, val loss 3.3463\n",
            " 3000 / 10000 : train loss 3.3101, val loss 3.3502\n",
            " 3500 / 10000 : train loss 3.3083, val loss 3.3474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Remimplemetation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(set(text))\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "vocab_size = len(chars)\n",
        "context_window = 128\n",
        "batch_size = 64\n",
        "emb_size = 256\n",
        "max_iters = 5000\n",
        "lr = 1e-3\n",
        "head_dim = 32\n",
        "n_head = emb_size // head_dim\n",
        "n_layer = 8\n",
        "\n",
        "itos = {i:v for i, v in enumerate(chars)}\n",
        "stoi = {v:i for i, v in enumerate(chars)}\n",
        "\n",
        "encode = lambda x : [stoi[i] for i in x]\n",
        "decode = lambda x : [itos[i] for i in x]\n",
        "\n",
        "enocded_data = torch.tensor(encode(text))\n",
        "\n",
        "def train_test_split():\n",
        "  n = int(len(enocded_data) * 0.9)\n",
        "  train = enocded_data[:n]\n",
        "  test = enocded_data[n:]\n",
        "  return train, test\n",
        "\n",
        "train, test = train_test_split()  # Remove\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train if split == 'train' else test\n",
        "  batch = torch.randint(0, len(data) - context_window, (batch_size,))\n",
        "  ix = torch.stack([data[i: i+context_window] for i in batch])\n",
        "  yx = torch.stack([data[i+1: i+context_window+1] for i in batch])\n",
        "  ix, yx = ix.to(device), yx.to(device)\n",
        "  return ix, yx\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(emb_size, head_dim)\n",
        "    self.key = nn.Linear(emb_size, head_dim)\n",
        "    self.value = nn.Linear(emb_size, head_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "\n",
        "    atten = (q @ k.transpose(-2, -1)) / (head_dim ** 0.5)\n",
        "    tril = torch.tril(torch.ones(context_window, context_window, device=device))\n",
        "    atten = atten.masked_fill(tril[:T, :T]==0, float('-inf'))\n",
        "    atten = F.softmax(atten, -1)\n",
        "    out = atten @ v\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head() for i in range(n_head)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], -1)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(emb_size, 4*emb_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*emb_size, emb_size),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sa_head = MultiHeadAttention()\n",
        "    self.lm_head = FeedForward()\n",
        "    self.ln1 = nn.LayerNorm(emb_size)\n",
        "    self.ln2 = nn.LayerNorm(emb_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    ln1_out = self.ln1(x)\n",
        "    sa_out = x + self.sa_head(ln1_out)\n",
        "    ln2_out = self.ln1(sa_out)\n",
        "    lm_out = x + self.lm_head(ln2_out)\n",
        "    return lm_out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.nn_emb = nn.Embedding(vocab_size, emb_size)\n",
        "    self.nn_pos = nn.Embedding(context_window, emb_size)\n",
        "    self.blocks = nn.Sequential(*[Block() for i in range(n_layer)])\n",
        "    self.nn_linear = nn.Linear(emb_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, y=None):\n",
        "    B, T = x.shape\n",
        "    emb = self.nn_emb(x)\n",
        "    pos_emb = self.nn_pos(torch.arange(T, device=device))\n",
        "    pos_emb = pos_emb + emb\n",
        "    block_out = self.blocks(pos_emb)\n",
        "    logits = self.nn_linear(block_out)\n",
        "\n",
        "    if y is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = y.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, ix, max_new_tokens):\n",
        "    for i in range(max_new_tokens):\n",
        "      ix_cond = ix[:, -context_window:]\n",
        "      logits, loss = self.forward(ix_cond)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits, -1)\n",
        "      ixn = torch.multinomial(probs, num_samples=1)\n",
        "      ix = torch.cat((ix, ixn), dim=1)\n",
        "    return ix\n",
        "\n",
        "model = BigramLanguageModel().to(device)\n",
        "print(f\"Total Parameters: {sum(i.numel() for i in model.parameters()):,}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "5vZ2aNduG2et",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bee33fe-3932-4535-ee04-e4136eee9992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Parameters: 5,857,857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(max_iters):\n",
        "  ix, yx = get_batch('train')\n",
        "  logits, loss = model(ix, yx)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  if i % 250 == 0:\n",
        "    print(f\"{i:5d} /  {max_iters} : loss is {loss:.4f}\")\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# Generation\n",
        "print(''.join(decode(model.generate(torch.tensor([[2]], device=device), 1000)[0].tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV1IUMQNR4ac",
        "outputId": "bc18c477-0fef-4cac-c386-ec122abe132d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0 /  5000 : loss is 4.5459\n",
            "  250 /  5000 : loss is 2.0996\n",
            "  500 /  5000 : loss is 1.7375\n",
            "  750 /  5000 : loss is 1.5076\n",
            " 1000 /  5000 : loss is 1.4073\n",
            " 1250 /  5000 : loss is 1.3664\n",
            " 1500 /  5000 : loss is 1.3197\n",
            " 1750 /  5000 : loss is 1.2654\n",
            " 2000 /  5000 : loss is 1.2172\n",
            " 2250 /  5000 : loss is 1.2118\n",
            " 2500 /  5000 : loss is 1.1514\n",
            " 2750 /  5000 : loss is 1.1526\n",
            " 3000 /  5000 : loss is 1.0938\n",
            " 3250 /  5000 : loss is 1.0523\n",
            " 3500 /  5000 : loss is 1.0299\n",
            " 3750 /  5000 : loss is 0.9833\n",
            " 4000 /  5000 : loss is 0.9563\n",
            " 4250 /  5000 : loss is 0.9371\n",
            " 4500 /  5000 : loss is 0.8295\n",
            " 4750 /  5000 : loss is 0.8154\n",
            "!\n",
            "\n",
            "KING EDWARD IV:\n",
            "Soft, tell me, is it doth. Answer I,\n",
            "Let it remembers smarves fetch at roar'd,\n",
            "As you shall lay the battle subject and deceive\n",
            "And dissolute your lusty for your debt.\n",
            "\n",
            "ROMEO:\n",
            "That she is tooth touch by my descent.\n",
            "Sir, bear the swines he knew my cousins\n",
            "Before that thy father rises with thy cheeks, which before\n",
            "My company may half made special we in our mistress.\n",
            "\n",
            "OXFERDIUS:\n",
            "Come, the fault\n",
            "That I know the chances of judgment did\n",
            "Dream out that which is a the house,\n",
            "Most dear, by the mire you almost for\n",
            "The labour in the plebeiring home. Sirrah, be gone\n",
            "To prevent the fiery eye:self our friends, the blood of\n",
            "Claudious lifer, whom a dispatch for a purpose\n",
            "Worth that point where the duke us innocenct.\n",
            "I hope it too. Where is that frighted,\n",
            "And when cease that broughts me shall both remove\n",
            "But from this turning to attendings:\n",
            "Some could ye it offended by a subject,\n",
            "She drink me frights my inforce times be with heaven,\n",
            "Not shall come to strange. Your nation\n",
            "Starts doings \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "4:10 - 4:25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc_jEXV_fxGP",
        "outputId": "77d61948-802a-4b91-9899-7dffd9dd7bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(12)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xt, yt = get_batch('test')\n",
        "logits, loss = model(xt, yt)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxQnwCQ1k1Ii",
        "outputId": "149a077b-e759-4a9b-e33e-bf562fc6fb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.9263, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8IA15Gm5BrRHRah5q/16z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}